import streamlit as st
import pandas as pd
import numpy as np


st.title("Projet Amazon Trustpilot")
st.sidebar.title("Sommaire")
pages=["Exploration", "Preprocessing", "Mod√©lisation"]
page=st.sidebar.radio("Aller vers", pages)


##############################################################
# Chargement du dataset
@st.cache_data
def load_dataset():
    df = pd.read_csv(
        "train.csv",
        header=None,
        names=["label", "title", "text"]
    )
    df["text"] = df["title"].fillna("") + " " + df["text"].fillna("")
    df_negative = df[df["label"] == 1]
    df_positive = df[df["label"] == 2]
    return df, df_negative, df_positive


df, df_negative, df_positive = load_dataset()


##############################################################
if page == pages[0]:

    # R√©partition des sentiments
    st.image(
        "images/repartition_sentiments.png",
        caption="Repartition des sentiments",
        use_container_width=True
    )
    # WordCloud
    st.image(
        "images/wordcloud.png",
        caption="Wordcloud ‚Äì Corpus global",
        use_container_width=True
    )





#############################################################
if page == pages[1] : 
    st.write("### Preprocessing")



#############################################################
if page == pages[2] : 
    st.write("### Mod√©lisation")

    ###########################################################################
    #   Pr√©diction du sentiment et du th√®me - 
    ###########################################################################
    import numpy as np
    import tensorflow as tf
    import joblib
    from transformers import (
        TFDistilBertForSequenceClassification,
        DistilBertTokenizerFast
    )
    from sentence_transformers import SentenceTransformer



    # =========================
    # Rechargement mod√®les
    # =========================
    @st.cache_resource
    def load_models():
        sbert_model = SentenceTransformer("../models/sentence_bert")
        kmeans = joblib.load("../models/kmeans_topics.pkl")
        cluster_labels = joblib.load("../models/cluster_labels.pkl")

        sentiment_model = TFDistilBertForSequenceClassification.from_pretrained(
            "../models/distilbert_sentiment"
        )
        sentiment_tokenizer = DistilBertTokenizerFast.from_pretrained(
            "../models/distilbert_sentiment"
        )

        return sbert_model, kmeans, cluster_labels, sentiment_model, sentiment_tokenizer
    
    sbert_model, kmeans, cluster_labels, sentiment_model, sentiment_tokenizer = load_models()


    # =========================
    # Fonction de pr√©diction
    # =========================
    def predict_review(review_text: str):

        # ---- Sentiment ----
        enc = sentiment_tokenizer(
            review_text,
            truncation=True,
            padding="max_length",
            max_length=256,
            return_tensors="tf"
        )

        outputs = sentiment_model(enc, training=False)
        probs = tf.nn.softmax(outputs.logits, axis=1).numpy()[0]

        sentiment = "Positive" if np.argmax(probs) == 1 else "Negative"

        # ---- Topic ----
        embedding = np.asarray(
            sbert_model.encode([review_text])
        )
        cluster_id = int(kmeans.predict(embedding)[0])
        theme = cluster_labels[cluster_id]

        return {
            "sentiment": sentiment,
            "sentiment_score": float(np.max(probs)),
            "theme": theme
        }


    # =========================
    # Test 1 reviews personnalis√©e
    # =========================
    review = "The movie stopped working after two weeks and feels very cheap."

    result = predict_review(review)

    st.markdown(f"### üìù Avis r√©dig√©")
    st.success(review)
    st.write(f"Th√®me :", result["theme"])
    st.write(f"Sentiment : {result['sentiment']}  ---   (Score = {result['sentiment_score']})")
    
    

    # =========================
    # Test de reviews du dataset
    # =========================
    # S√©lection al√©atoire de 5 avis positifs et 5 avis n√©gatifs
    random_state = 43
    def safe_sample(df, n, random_state):
        if len(df) == 0:
            return pd.DataFrame(columns=df.columns)
        return df.sample(
            n=min(n, len(df)),
            random_state=random_state
        )

    neg_samples = safe_sample(df_negative, 5, random_state)[["text", "label"]]
    pos_samples = safe_sample(df_positive, 5, random_state)[["text", "label"]]


    # Dataset de test final (m√©lang√©)
    test_df = (
        pd.concat([neg_samples, pos_samples])
        .sample(frac=1, random_state=random_state)
        .reset_index(drop=True)
    )

    # Correspondance labels
    label_mapping = {
        1: "Negative",
        2: "Positive"
    }

    # Pr√©dictions
    for i, row in test_df.iterrows():
        review_text = row["text"]
        true_label = label_mapping[row["label"]]

        result = predict_review(review_text)

        st.markdown(f"### üìù Avis {i+1}")
        st.info(review_text)

        st.write(f"‚Üí Sentiment r√©el   : **{true_label}**")
        st.write(
            f"‚Üí Sentiment pr√©dit : **{result['sentiment']}** "
            f"(score = {result['sentiment_score']:.3f})"
        )
        st.write(f"‚Üí Th√®me pr√©dit     : **{result['theme']}**")

        st.divider()

